{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b983189-a57d-4f14-a931-f993fa1b8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9974396084348964\n",
      "Confusion Matrix:\n",
      " [[386538    213]\n",
      " [   783   1469]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    386751\n",
      "           1       0.87      0.65      0.75      2252\n",
      "\n",
      "    accuracy                           1.00    389003\n",
      "   macro avg       0.94      0.83      0.87    389003\n",
      "weighted avg       1.00      1.00      1.00    389003\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "       Feature  Importance\n",
      "1         amt    0.709995\n",
      "0    category    0.125265\n",
      "5   merch_lat    0.034422\n",
      "6  merch_long    0.034144\n",
      "7         age    0.033991\n",
      "3    city_pop    0.031271\n",
      "4         job    0.024700\n",
      "2      gender    0.006213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# 1. Read the data (replace `data.csv` with your file)\n",
    "df = pd.read_csv(\"train_cleaned.csv\")\n",
    "\n",
    "# 2. Data preprocessing\n",
    "# Convert date columns\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Create an age column from the birthdate\n",
    "df['age'] = (df['trans_date_trans_time'].dt.year - df['dob'].dt.year)\n",
    "\n",
    "# Drop unnecessary or difficult-to-use columns\n",
    "columns_to_drop = ['trans_date_trans_time', 'cc_num', 'first', 'last', \n",
    "                   'street', 'city', 'state', 'zip', 'lat', 'long', 'dob', \n",
    "                   'trans_num', 'unix_time', 'merchant']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in ['category', 'gender', 'job']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Standardize numerical variables\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = ['amt', 'city_pop', 'merch_lat', 'merch_long', 'age']\n",
    "df[scaled_columns] = scaler.fit_transform(df[scaled_columns])\n",
    "\n",
    "# 3. Split the data into training and test sets\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 6. Observe feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b14527-b89d-49ea-a2d2-c2df212e0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best parameters found:  {'classifier__max_depth': 9, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 9, 'classifier__n_estimators': 93}\n",
      "\n",
      "Accuracy:  0.9722315766202317\n",
      "Confusion Matrix:\n",
      " [[376474  10277]\n",
      " [   525   1727]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99    386751\n",
      "           1       0.14      0.77      0.24      2252\n",
      "\n",
      "    accuracy                           0.97    389003\n",
      "   macro avg       0.57      0.87      0.61    389003\n",
      "weighted avg       0.99      0.97      0.98    389003\n",
      "\n",
      "\n",
      "Cross-validation accuracy scores: [0.97100584 0.97182255 0.97553358]\n",
      "Mean cross-validation accuracy:  0.9727873214182429\n",
      "\n",
      "Feature Importances:\n",
      "                                  Feature  Importance\n",
      "0                                    amt    0.480338\n",
      "16                 category_shopping_net    0.076623\n",
      "9                   category_grocery_pos    0.061309\n",
      "11                         category_home    0.046665\n",
      "10               category_health_fitness    0.040100\n",
      "..                                   ...         ...\n",
      "256                   job_Hydrogeologist    0.000000\n",
      "254                    job_Hotel manager    0.000000\n",
      "253              job_Hospital pharmacist    0.000000\n",
      "235  job_Geophysicist/field seismologist    0.000000\n",
      "257            job_Hydrographic surveyor    0.000000\n",
      "\n",
      "[515 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import randint\n",
    "import joblib\n",
    "\n",
    "# 1. Read the data (replace `data.csv` with your file)\n",
    "df = pd.read_csv(\"train_cleaned.csv\")\n",
    "\n",
    "# 2. Process the data\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "df['age'] = (df['trans_date_trans_time'].dt.year - df['dob'].dt.year)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['trans_date_trans_time', 'cc_num', 'first', 'last', \n",
    "                   'street', 'city', 'state', 'zip', 'lat', 'long', 'dob', \n",
    "                   'trans_num', 'unix_time', 'merchant']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Create steps in the Pipeline\n",
    "categorical_columns = ['category', 'gender', 'job']\n",
    "numerical_columns = ['amt', 'city_pop', 'merch_lat', 'merch_long', 'age']\n",
    "\n",
    "# Preprocessing for categorical and numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),  # Scale numerical columns\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)  # One-hot encode categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Build Pipeline with preprocessing step and RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Apply preprocessing to the data\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1))  # Random Forest model\n",
    "])\n",
    "\n",
    "# 5. Hyperparameter search using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': randint(50, 100),  # Reduce the number of trees from 50 to 100\n",
    "    'classifier__max_depth': randint(5, 15),  # Reduce the maximum depth of the trees\n",
    "    'classifier__min_samples_split': randint(2, 10),  # Reduce the minimum number of samples to split\n",
    "    'classifier__min_samples_leaf': randint(1, 10)  # Reduce the minimum number of samples at a leaf\n",
    "}\n",
    "\n",
    "# Reduce the number of parameter trials and splits (n_iter=5, cv=3)\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, \n",
    "                                   n_iter=5, cv=3, random_state=42, n_jobs=-1, verbose=1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Cross-validation\n",
    "cv_scores = cross_val_score(random_search.best_estimator_, X, y, cv=3, scoring='accuracy')  # Use 3-fold cross-validation\n",
    "print(\"\\nCross-validation accuracy scores:\", cv_scores)\n",
    "print(\"Mean cross-validation accuracy: \", np.mean(cv_scores))\n",
    "\n",
    "# 8. Feature Selection\n",
    "# Apply SelectFromModel on the pipeline after training\n",
    "selector = SelectFromModel(random_search.best_estimator_.named_steps['classifier'], threshold=\"mean\", max_features=10)\n",
    "\n",
    "# Pass the preprocessed data into the selector to select important features\n",
    "X_train_selected = random_search.best_estimator_.named_steps['preprocessor'].transform(X_train)\n",
    "X_train_selected = selector.transform(X_train_selected)\n",
    "\n",
    "# 9. Save the trained model with joblib to save time for next use\n",
    "joblib.dump(random_search.best_estimator_, 'random_forest_model.pkl')\n",
    "\n",
    "# 10. Observe feature importances\n",
    "best_rf_model = random_search.best_estimator_.named_steps['classifier']\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': numerical_columns + list(random_search.best_estimator_.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_columns)),\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fb0ca-ee0d-40b1-9037-ba8c423470b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
